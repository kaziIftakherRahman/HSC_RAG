# HSC Bangla Literature RAG API

This project is a Retrieval-Augmented Generation (RAG) system designed to answer questions about HSC-level Bangla literature. It processes a PDF of a Bangla story called "অপরিচিতা", extracts the text using OCR, and uses a conversational AI chain powered by Google's Gemini models to answer user queries based on the text.

---

## Features

- **PDF Processing**: Converts specified pages from a PDF into images for text extraction.
- **Advanced OCR**: Uses Tesseract with custom image pre-processing (sharpening, contrast) to accurately extract Bengali text.
- **Vector Storage**: Embeds text chunks and stores them in a ChromaDB vector store.
- **Conversational RAG Chain**: Uses LangChain and Google's Gemini models to generate answers based on retrieved context, maintaining chat history.
- **FastAPI Endpoint**: Exposes the conversational chain via an API endpoint, made publicly accessible with ngrok.

---

## Setup Guide

1.  **Clone the Repository**
    ```bash
    git clone [your-github-repo-url]
    cd [your-repo-name]
    ```

2.  **Install Dependencies**
    The system requires several Python packages and system libraries. You can install them using pip and apt.
    ```bash
    # Python Packages
    pip install langchain langchain-community langchain-google-genai chromadb llama-index pdf2image pytesseract unstructured tiktoken opencv-python-headless fastapi uvicorn pyngrok

    # System Libraries (for OCR and PDF processing)
    sudo apt-get update
    sudo apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-ben
    ```

3.  **Set Up API Keys**
    This project requires a Google Gemini API key and an ngrok authentication token.
    - In your Google Colab environment, store your Gemini API key as a secret named `GOOGLE_API_KEY_1`.
    - In the API code block, replace `"30Mp4YhMiIT6evuX74fObq8eISF_6dRn6Qz8sYvnBQBdMrUat"` with your ngrok authentication token.

4.  **Add Data**
    Place your PDF file in the `/content/` directory and ensure its name is `HSC26-Bangla1st-Paper.pdf`, as this path is hardcoded in the notebook.

5.  **Run the Application**
    Execute the cells in the Jupyter Notebook (`api_HSC_RAG.ipynb`) in sequential order. The final cell will start the FastAPI server and provide a public ngrok URL to access the API.

---

## Used Tools, Libraries, and Packages

-   **Core AI/ML**: `langchain`, `google-generativeai`, `llama-index`, `pytorch`
-   **Vector Database**: `chromadb`
-   **Web Framework**: `fastapi`, `uvicorn`
-   **PDF/Image Processing**: `pdf2image`, `pytesseract`, `opencv-python-headless`, `Pillow`
-   **System Tools**: `poppler-utils`, `tesseract-ocr`
-   **Tunneling**: `pyngrok`

---

## Sample Queries

Here are some examples of questions (in both Bangla and English) and the answers generated by the system.

| Query | Answer |
| :--- | :--- |
| অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে? | অনুপমের ভাষায় সুপুরুষ বলা হয়েছে শস্তুনাথবাবুকে। গল্পে তার চেহারার বর্ণনা দিতে গিয়ে বলা হয়েছে, "বয়স তার চল্লিশের কিছু এপারে বা ওপারে। চুল কীচা, গোঁফে পাক ধরিতে আরম্ভ করিয়াছে মাত্র। সুপুরুষ বটে। ভিড়ের মধ্যে দেখিলে সকলের আগে তার উপরে চোখ পড়িবার মতো চেহারা।" |
| Who is Harish? | হরিশ হলেন একজন ব্যক্তি যিনি আসর জমাতে অদ্বিতীয় এবং সর্বত্র তার খাতির ছিল। |

--- 

## API Documentation

The API provides a single endpoint for interacting with the RAG chain.

### POST `/chat`

Handles a single turn in a conversation. It requires a session ID to maintain chat history.

**Request Body:**

```json
{
  "session_id": "string",
  "question": "string"
}
```

## Evaluation Matrix

The RAG pipeline was evaluated using the `ragas` framework on a small, curated dataset of questions and ground-truth answers. The results are as follows:

| Question | Answer | Ground Truth | Context Precision | Context Recall | Faithfulness | Answer Relevancy |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে? | অনুপমের ভাষায় সুপুরুষ বলা হয়েছে শস্তুনাথবাবুকে... | শম্ভুনাথ | 0.277 | 1.0 | 1.0 | 0.723 |
| কাকে অনুপমের/লেখকের ভাগ্যদেবতা বলে উল্লেখ করা হয়েছে? | অনুুপমের/লেখকের মামাকে তার ভাগ্যদেবতা বলে... | মামা | 0.267 | 1.0 | 1.0 | 0.800 |
| বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল? | তথ্যসূত্র অনুযায়ী, কল্যাণীর বয়স ষোলো কি সতেরো... | পনেরো | 0.250 | 1.0 | 0.733 | 0.568 |

### Summary of Results:

The evaluation shows that the system is excellent at finding the correct information (**`context_recall`** = 1.0) and sticking to the facts it finds (**`faithfulness`** ≈ 1.0). However, the very low **`context_precision`** scores indicate that the retriever fetches too much irrelevant information, which negatively impacts the final **`answer_relevancy`**.

---

## Answer of the Required Questions

### **1. What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?**

I managed to employ a two-stage approach to extracting text. To process a PDF document in such a way I applied the python library `pdf2image` that converts definite pages of this document into high-resolution PNG images. Afterwards, I applied Optical Character Recognition (OCR) on these images, using the library called `pytesseract` whose author is Michal Csoka Toth and this is a Python wrapper of Google Tesseract-OCR Engine, and then, I extracted the Bengali text.

The reason to opt for this is that the source PDF is a scanned document, and not a digitally native text file. It is not copy-paste friendly and therefore OCR was the only option of extraction.

Moreover, I faced problems dealing with multicolored text, I solved it by separating the background color white from any other color presented in the pdf.

I had serious formatting problems. The output of OCR was distorted and lined with noise. I used OpenCV (`cv2`) and Pillow (`PIL`) to alleviate this in a form of an image pre-processing step. I changed the image colors into another color space, used a sharpening kernel and contrast. This improvement helped Tesseract read the text more clearly, which in turn gave an enormous boost in the accuracy of the extracted Bengali text.

### **2. What chunking strategy did you choose? Why do you think it works well for semantic retrieval?**

I did a character-limit-based chunking approach with LangChain functons `RecursiveCharacterTextSplitter`. It was configured as `chunk_size=1000 characters` and `chunk_overlap=200 char`.

Such an approach is effective in satisfying semantic retrieval in two fundamental reasons:
1. Preserves Cohesion: The `RecursiveCharacterTextSplitter` is perfect since it does not merely divide the text itself at the boundary of characters without rhyme or reason. It initially attempts to break on natural semantic lines such as paragraphs (`\n\n`), sentences (`.`), etc. This leaves related sentences within a chunk and allows the meaning to be kept in tact.
2. Avoids Loss of context: The 200 character chunk_overlap is important. In case one sentence or an important idea occurs at the point where a chunk is divided, the overlap means the entire idea is present somewhere in one of the chunks and no context is lost at the boundaries.

### **3. What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?**

I utilized the `models/text-embedding-004` Google model, which was available through the special `GoogleGenerativeAIEmbeddings` class offered in LangChain.

The reason I selected this model is that it is a powerful and a state-of-the-art embedding model which is compatible with Google Gemini family of generative models that I selected to generate the final answer. Same ecosystem models usually guarantee higher performance and compatibility. It is also multilingual in its capabilities and thus is quite compatible to act as the perfect software in the processing of the Bengali text of the source-document.

The model encodes the meaning of the text, by encoding it in terms of a high-dimensional numerical vector. It is a transformer based-model that had been trained on a huge amount of data and thus manages to comprehend complicated semantic connections, context and subtlety. In this vector space words and sentences that have similar meanings are assigned points that are nearby in this sense.

### **4. How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?**

This is compared by use of vector similarity search. That is how it works:
1. Storage: The chunks of documents are turned into embeddings (vectors), and the embeddings can be stored in a ChromaDB vector store. The main reasons I chose ChromaDB were that it is lightweight, performant and very simple to operationalize; particularly following an in-memory format as in the case in this project.
2. Comparison: Upon a query made by a user, it is translated to an embedding as well. A similarity search is then carried out in chromeDB to retrieve the stored chunk vectors which are nearest to the query vector. The method used behind this type of comparison is usually Cosine Similarity and calculates the angle of two vectors and thus how similar they are in meaning.

I selected this type of search that involves vector since this is the common and the most adopted practice of semantic search. It extends more simple keyword matching and locates chunks that are semantically and contextually related to a query, which is what is necessary in a high quality RAG system.

### **5. How do you ensure that the question and the document chunks are compared meaningfully? What would happen if the query is vague or missing context?**

Any meaningful comparison is guaranteed by the simple general principles, in that the same embedding model (`text-embedding-004`) is used to both encode the document fragments in the storage time, as well as the user questions, at the query time. This ensures that the documents and the questions are all projected in a high-dimensional vector space. When they lie in the same semantic space, the angle and distance between their respective vectors will give a usable measure of the similarity of their context.

An ambiguous or otherwise un-contexted query (e.g., tell me more about his thoughts) will have a generic resulting vector. Upon the retriever searching the nearest vectors in the database it is probable that it will retrieve chunks that are generally the same or an average of some topics that have been stated in the text. This garbage in-garbage out basis implies that the context retrieved will present poor results, and the resulting final LLM will either:
Give an extremely generic and counterproductive response.
Say that it does not know the answer to the question on the basis of the information.
Hallucinate a correct response on the basis of the confused context.

### **6. Do the results seem relevant? If not, what might improve them?**

According to the evaluation matrix, the outcomes are somewhat relevant and can use some improvement.

The 1.0 `context_recall` score is outstanding, meaning that the retriever can retrieve the right document. but the score `context_precision` is extremely low (valued around 0.25-0.28) indicating that it is returning a bunch of irrelevant data along with the correct chunks. The negative effect of this "noise" is reflected in the average scores of `answer_relevancy` and one of the answers detailing "কল্যাণীর বয়স" was inaccurate. I did a few trial and errors but failed to extract the exact ansswer "পনেরো" 

The following would help to enhance the outcomes:
1. Minimize the `k` Value: The most consequential alteration to make would decrease the number of documents retrieved. The retriever will fetch `k=15` chunks which is excessive and saturates the context with noise. A decrease in this to a smaller set, such as `k=3` or `k=5`, would probably increase precision by a wide margin.
2. Enhance Chunking Strategy: A smaller `chunk_size` might yield more semantically specific chunks, only the most relevant of which the retriever would be able to choose.
3. Prompt Engineering: The system prompt may be improved by adding an instruction that encourages the LLM to more forcefully rule out irrelevant information in the information provided when answering the question.
